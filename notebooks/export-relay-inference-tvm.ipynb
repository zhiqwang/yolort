{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying yolort on TVM\n",
    "\n",
    "This article is an introductory tutorial to deploy PyTorch YOLOv5 models with Relay VM.\n",
    "\n",
    "For us to begin with, PyTorch should be installed.\n",
    "TorchVision is also required since we will be using it as our model zoo.\n",
    "\n",
    "A quick solution is to install via pip\n",
    "\n",
    "\n",
    "```shell\n",
    "pip install torch==1.7.1\n",
    "pip install torchvision==0.8.2\n",
    "```\n",
    "\n",
    "or please refer to official site\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "PyTorch versions should be backwards compatible but should be used\n",
    "with the proper TorchVision version.\n",
    "\n",
    "Currently, Only test `TVM` with PyTorch 1.7. Other versions may be unstable.\n",
    "\n",
    "---\n",
    "\n",
    "Copyright © Most of the codes is copied from the [TVM tutorial](https://tvm.apache.org/docs/tutorials/frontend/deploy_object_detection_pytorch.html#sphx-glr-tutorials-frontend-deploy-object-detection-pytorch-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.runtime.vm import VirtualMachine\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained `yolov5s` from yolort and do tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 416\n",
    "input_shape = (in_size, in_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolort.models import yolov5s\n",
    "from yolort.relaying import get_trace_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/tensor.py:593: RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  'incorrect results).', category=RuntimeWarning)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/anchor_utils.py:31: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  stride = torch.as_tensor([stride], dtype=dtype, device=device)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/anchor_utils.py:50: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  anchor_grid = torch.as_tensor(anchor_grid, dtype=dtype, device=device)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/anchor_utils.py:79: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  shifts = shifts - torch.tensor(0.5, dtype=shifts.dtype, device=device)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/box_head.py:332: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  for idx in range(batch_size):  # image idx, image inference\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/transform.py:298: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  for s, s_orig in zip(new_size, original_size)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/transform.py:298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  for s, s_orig in zip(new_size, original_size)\n"
     ]
    }
   ],
   "source": [
    "model_func = yolov5s(pretrained=True)\n",
    "script_module = get_trace_module(model_func, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load with following\n",
    "\n",
    "```python\n",
    "model = torch.hub.load('zhiqwang/yolov5-rt-stack', 'yolov5s', pretrained=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.yolort.relaying.trace_wrapper.TraceWrapper,\n",
       "      %images : Float(1:519168, 3:173056, 416:416, 416:1, requires_grad=0, device=cpu)):\n",
       "  %4399 : __torch__.yolort.models.yolo_module.YOLOModule = prim::GetAttr[name=\"model\"](%self.1)\n",
       "  %4778 : (Tensor, Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%4399, %images)\n",
       "  %4775 : Float(14:4, 4:1, requires_grad=0, device=cpu), %4776 : Float(14:1, requires_grad=0, device=cpu), %4777 : Long(14:1, requires_grad=0, device=cpu) = prim::TupleUnpack(%4778)\n",
       "  %3515 : (Float(14:4, 4:1, requires_grad=0, device=cpu), Float(14:1, requires_grad=0, device=cpu), Long(14:1, requires_grad=0, device=cpu)) = prim::TupleConstruct(%4775, %4776, %4777)\n",
       "  return (%3515)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_module.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a test image and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolort.utils import get_image_from_url\n",
    "\n",
    "img = get_image_from_url(\"https://gitee.com/zhiqwang/yolov5-rt-stack/raw/master/test/assets/bus.jpg\")\n",
    "# img = cv2.imread('../test/assets/bus.jpg')\n",
    "\n",
    "img = img.astype(\"float32\")\n",
    "img = cv2.resize(img, (in_size, in_size))\n",
    "\n",
    "img = np.transpose(img / 255.0, [2, 0, 1])\n",
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the graph to Relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n",
      "WARNING:root:Untyped Tensor found, assume it is float32\n"
     ]
    }
   ],
   "source": [
    "input_name = \"input0\"\n",
    "shape_list = [(input_name, (1, 3, *input_shape))]\n",
    "mod, params = relay.frontend.from_pytorch(script_module, shape_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile with Relay VM\n",
    "\n",
    "Note: Currently only CPU target is supported. For x86 target, it is\n",
    "highly recommended to build TVM with Intel MKL and Intel OpenMP to get\n",
    "best performance, due to the existence of large dense operator in\n",
    "torchvision rcnn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "# Add \"-libs=mkl\" to get best performance on x86 target.\n",
    "# For x86 machine supports AVX512, the complete target is\n",
    "# \"llvm -mcpu=skylake-avx512 -libs=mkl\"\n",
    "target = \"llvm\"\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3, disabled_pass=[\"FoldScaleAxis\"]):\n",
    "    vm_exec = relay.vm.compile(mod, target=target, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Relay VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ctx = tvm.cpu()\n",
    "vm = VirtualMachine(vm_exec, ctx)\n",
    "vm.set_input(\"main\", **{input_name: img})\n",
    "tvm_res = vm.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.6 ms ± 1.52 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "vm.set_input(\"main\", **{input_name: img})\n",
    "tvm_res = vm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get boxes with score larger than 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 3 valid boxes\n"
     ]
    }
   ],
   "source": [
    "score_threshold = 0.6\n",
    "boxes = tvm_res[0].asnumpy().tolist()\n",
    "valid_boxes = []\n",
    "for i, score in enumerate(tvm_res[1].asnumpy().tolist()):\n",
    "    if score > score_threshold:\n",
    "        valid_boxes.append(boxes[i])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"Get {len(valid_boxes)} valid boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varify the Inference Output on TVM backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch_res = script_module(torch.from_numpy(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with TVM Runtime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(torch_res)):\n",
    "    torch.testing.assert_allclose(torch_res[i], tvm_res[i].asnumpy(), rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with TVM Runtime, and the result looks good!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
