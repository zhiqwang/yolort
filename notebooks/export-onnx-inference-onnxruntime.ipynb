{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying yolort on ONNXRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "from yolort.models import yolov5s\n",
    "\n",
    "from yolort.utils import get_image_from_url, read_image_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = yolov5s(export_friendly=True, pretrained=True, score_thresh=0.45)\n",
    "\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images to infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_one = get_image_from_url(\"https://gitee.com/zhiqwang/yolov5-rt-stack/raw/master/test/assets/bus.jpg\")\n",
    "# img_one = cv2.imread('../test/assets/bus.jpg')\n",
    "img_one = read_image_to_tensor(img_one, is_half=False)\n",
    "img_one = img_one.to(device)\n",
    "\n",
    "img_two = get_image_from_url(\"https://gitee.com/zhiqwang/yolov5-rt-stack/raw/master/test/assets/zidane.jpg\")\n",
    "# img_two = cv2.imread('../test/assets/zidane.jpg')\n",
    "img_two = read_image_to_tensor(img_two, is_half=False)\n",
    "img_two = img_two.to(device)\n",
    "\n",
    "# images = [img_one, img_two]\n",
    "# Uncomment the above line and comment the next line if you want to\n",
    "# use the multi-batch inferencing on onnxruntime\n",
    "images = [img_one]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on PyTorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 20 ms, total: 3.46 s\n",
      "Wall time: 96.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    model_out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[669.26556, 391.30249, 809.86627, 885.23444],\n",
       "        [ 54.06350, 397.83176, 235.95316, 901.37323],\n",
       "        [222.88336, 406.81192, 341.55716, 854.77924],\n",
       "        [ 18.63205, 232.97676, 810.97394, 760.11700]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out[0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.89005, 0.87333, 0.85366, 0.72340])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops._register_onnx_ops import _onnx_opset_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using opset version: 11\n"
     ]
    }
   ],
   "source": [
    "export_onnx_name = 'yolov5s.onnx'  # path of the exported ONNX models\n",
    "\n",
    "print(f'We are using opset version: {_onnx_opset_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py:1192: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input images_tensors\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py:1192: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input boxes\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py:1192: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input labels\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py:1192: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input scores\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/anchor_utils.py:31: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  stride = torch.as_tensor([stride], dtype=dtype, device=device)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/anchor_utils.py:50: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  anchor_grid = torch.as_tensor(anchor_grid, dtype=dtype, device=device)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/anchor_utils.py:79: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  shifts = shifts - torch.tensor(0.5, dtype=shifts.dtype, device=device)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/transform.py:298: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  for s, s_orig in zip(new_size, original_size)\n",
      "/data/wangzq/yolov5-rt-stack/yolort/models/transform.py:298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  for s, s_orig in zip(new_size, original_size)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_opset9.py:2766: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  \"If indices include negative values, the exported graph will produce incorrect results.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_opset9.py:701: UserWarning: This model contains a squeeze operation on dimension 1 on an input with unknown shape. Note that if the size of dimension 1 of the input is not 1, the ONNX model will return an error. Opset version 11 supports squeezing on non-singleton dimensions, it is recommended to export this model using opset version 11 or higher.\n",
      "  \"version 11 or higher.\")\n"
     ]
    }
   ],
   "source": [
    "# Export to ONNX model\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (images,),\n",
    "    export_onnx_name,\n",
    "    do_constant_folding=True,\n",
    "    opset_version=_onnx_opset_version, \n",
    "    input_names=[\"images_tensors\"],\n",
    "    output_names=[\"scores\", \"labels\", \"boxes\"],\n",
    "    dynamic_axes={\n",
    "        \"images_tensors\": [0, 1, 2],\n",
    "        \"boxes\": [0, 1],\n",
    "        \"labels\": [0],\n",
    "        \"scores\": [0],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify the exported ONNX model (Optional)\n",
    "\n",
    "*ONNX* is great, but sometimes too complicated. And thanks to @daquexian for providing a powerful tool named [`onnxsim`](https://github.com/daquexian/onnx-simplifier/) to eliminate some redundant operators.\n",
    "\n",
    "First of all, let's install `onnx-simplifier` with following script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "pip install -U onnx-simplifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simplifing with onnxsim 0.3.6\n"
     ]
    }
   ],
   "source": [
    "import onnxsim\n",
    "\n",
    "# onnx-simplifier version\n",
    "print(f'Starting simplifing with onnxsim {onnxsim.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_simp_name = 'yolov5s.simp.onnx'  # path of the simplified ONNX models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your predefined ONNX model\n",
    "onnx_model = onnx.load(export_onnx_name)\n",
    "\n",
    "# convert model\n",
    "model_simp, check = onnxsim.simplify(\n",
    "    onnx_model,\n",
    "    input_shapes={\"images_tensors\": [3, 640, 640]},\n",
    "    dynamic_input_shape=True,\n",
    ")\n",
    "\n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "\n",
    "# use model_simp as a standard ONNX model object\n",
    "onnx.save(model_simp, onnx_simp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on ONNXRuntime Backend\n",
    "\n",
    "Now, We begin to verify whether the inference results are consistent with PyTorch's, similarly, install `onnxruntime` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "pip install -U onnxruntime\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with onnx 1.9.0, onnxruntime 1.8.1...\n"
     ]
    }
   ],
   "source": [
    "print(f'Starting with onnx {onnx.__version__}, onnxruntime {onnxruntime.__version__}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = torch.jit._flatten(images)\n",
    "outputs, _ = torch.jit._flatten(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    if tensor.requires_grad:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = list(map(to_numpy, images))\n",
    "outputs = list(map(to_numpy, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ort_session = onnxruntime.InferenceSession(export_onnx_name)\n",
    "ort_session = onnxruntime.InferenceSession(onnx_simp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute onnxruntime output prediction\n",
    "ort_inputs = dict((ort_session.get_inputs()[i].name, inpt) for i, inpt in enumerate(inputs))\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.38 s, sys: 20 ms, total: 2.4 s\n",
      "Wall time: 65.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute onnxruntime output prediction\n",
    "ort_inputs = dict((ort_session.get_inputs()[i].name, inpt) for i, inpt in enumerate(inputs))\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(outputs)):\n",
    "    torch.testing.assert_allclose(outputs[i], ort_outs[i], rtol=1e-04, atol=1e-07)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv5 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
